{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA(Latent Dirichlet Allocation)\n",
    "## 토픽은 각각 단어 분포를 가지고 있다고 가정한다. \n",
    "### 1. n개의 토픽을 지정한다. \n",
    "### 2. 무작위로 토픽의 혼합 비율을 설정\n",
    "### 3. 단어들을 토픽에 할당\n",
    "####### 현재 문서의 모든 단어를 토픽에 할당\n",
    "####### 현재 문서에서 토픽 비율 업데이트\n",
    "####### 모든 문서에서 토픽 비율 업데이트\n",
    "### 4. 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"./data/appreply2.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 2. 텍스트->숫자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1) 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표\n",
    "# 나는 밥을 먹었다 -> (토크나이징) 나 / 는 / 밥 / 을 / 먹었다. -> 나 는 밥 을 먹었다.\n",
    "# 단어들이 띄어쓰기를 구분자로 하여 하나의 문자열로 완성되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiwipiepy 품사 태그 목록 https://bab2min.github.io/kiwipiepy/v0.22.2/kr/#_11\n",
    "\n",
    "# STEP1. 전처리한 문장을 담을 빈리스트를 준비한다. sent_list \n",
    "# STEP2. df[\"text\"]에서 문장을 하나씩 뺀다. sent \n",
    "# STEP3. 문장을 정규표현식을 이용해서 전처리한다. clean_sent\n",
    "# STEP4. clean_sent를 토크나이징한다. (Kiwi 형태소 분석기 사용) result\n",
    "         # 1) 조건 : 조사(J), 어미(E), 접미사(X)는 포함하지 않는다.\n",
    "         # 2) 조건 : 한 글자인 단어는 포함하지 않는다.\n",
    "# STEP5. result를 \" \"를 구분자로 하나의 문자열로 합친다. result_str\n",
    "# STEP6. result_str을 sent_list에 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from kiwipiepy import Kiwi \n",
    "\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = kiwi.tokenize(\"안녕하세요. 저는 형태소 분석기에요.\")\n",
    "# print(sample_test)\n",
    "for x in sample_test:\n",
    "    print(f\"x = {x}\")\n",
    "    print(f\"word = {x.form}, pos = {x.tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP1. 전처리한 문장을 담을 빈리스트를 준비한다. sent_list \n",
    "sent_list = []\n",
    "\n",
    "# STEP2. df[\"text\"]에서 문장을 하나씩 뺀다. sent \n",
    "for sent in df[\"text\"]:\n",
    "    # STEP3. 문장을 정규표현식을 이용해서 전처리한다. clean_sent\n",
    "    print(\"STEP3. 문장을 전처리합니다.\")\n",
    "    clean_sent = re.sub(\"[^a-zA-Z가-힣\\s]\", \"\", sent)\n",
    "    # STEP4. clean_sent를 토크나이징한다. (Kiwi 형태소 분석기 사용) result\n",
    "    print(\"STEP4. 문장을 토크나이징합니다.\")\n",
    "    result = kiwi.tokenize(clean_sent)\n",
    "    print(\"\\tsub_list를 생성합니다.\")\n",
    "    sub_list = []\n",
    "    print(\"\\t탐색을 시작합니다.\")\n",
    "    for x in result:\n",
    "        # 1) 조건 : 조사(J), 어미(E), 접미사(X)는 포함하지 않는다. -> pos[0] in [\"J\", \"E\", \"X\"] 건너뛰기\n",
    "        word = x.form\n",
    "        pos = x.tag \n",
    "        if pos[0] in [\"J\", \"E\", \"X\"]:\n",
    "            # print(f\"\\t건너뛰기!! word={word} pos={pos}\")\n",
    "            continue\n",
    "        # 2) 조건 : 한 글자인 단어는 포함하지 않는다.\n",
    "        if len(word) > 1:\n",
    "            sub_list.append(word)\n",
    "        # print(f\"\\t추가!! word={word} pos={pos}\")\n",
    "    print(\"\\t탐색을 종료합니다.\")\n",
    "    print(f\"[SUB LIST] {sub_list}\")\n",
    "    # STEP5. result를 \" \"를 구분자로 하나의 문자열로 합친다. result_str\n",
    "    result_str = \" \".join(sub_list)\n",
    "    print(f\"[RESULT_STR] {result_str}\")\n",
    "    # STEP6. result_str을 sent_list에 넣는다.\n",
    "    sent_list.append(result_str)\n",
    "    print(sent)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sent_list))\n",
    "for old, new in zip(df[\"text\"].tolist(), sent_list):\n",
    "    print(f\"[OLD] {old}\")\n",
    "    print(f\"[NEW] {new}\")\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 2) 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치\n",
    "# uv add scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer란?\n",
    "# 문장에 해당 단어가 몇 개 들어가 있나요?\n",
    "#      단어1  단어2  단어3 ...\n",
    "# 문장1\n",
    "# 문장2 \n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.1,         # 전체 단어의 등장 비율이 p이상인 것만 사용\n",
    "    min_df=2,           # 이 단어가 적어도 n개 이상 있는 것만 사용\n",
    "    max_features=1000,  # 최대 몇 개까지 나타낼 것인가\n",
    "    ngram_range=(1,2)   # 단어의 조합 설정(ex. 1개만 사용)\n",
    ")\n",
    "feat_vec = count_vectorizer.fit_transform(sent_list)\n",
    "# 문장 -> 스페이스 기준으로 쪼갠다.  -> 단어들이 나열\n",
    "# 각 문장마다 단어가 몇번 들어가 있는지 카운트를 세고\n",
    "# 하나의 매트릭스로 표현한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Sparse Matrix를 일반 배열(Dense)로 변환: .toarray()\n",
    "# 2. 열 이름(단어 목록) 가져오기: get_feature_names_out()\n",
    "df_vec = pd.DataFrame(\n",
    "    feat_vec.toarray(), \n",
    "    columns=count_vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "df_vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# 3. 토픽분석 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 설치\n",
    "# uv add pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5) # 몇 개의 topic으로 할까요?\n",
    "lda.fit(feat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.lda_model \n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.lda_model.prepare(lda, feat_vec, count_vectorizer)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행: 문장 \n",
    "# 열: 그룹(n_component)\n",
    "# 값: 문장이 그룹에 포함될 확률\n",
    "# ex. 문장1  [0.1  0.8  0.1]  -> 이 문장1은 그룹 2이다. \n",
    "sent_topic = lda.transform(feat_vec)\n",
    "print(sent_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "sent1 = np.array([0.01300719, 0.976353, 0.01063981])\n",
    "print(sent1.sum())\n",
    "print(sent1.max())        # 배열의 최댓값\n",
    "print(sent1.argmax())     # 배열에 최댓값이 있는 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "# STEP0. 빈 리스트 2개를 준비한다. - topic_idx_list, topic_prob_list\n",
    "# STEP1. sent_topic에서 요소 하나하나씩 뽑는다. [0.01300719, 0.976353, 0.01063981]\n",
    "# STEP2. 이 문장을 어떤 그룹으로 분류할 건지(argmax()) - topic_idx\n",
    "# STEP3. 이 문장이 분류되었을 때 그 확률(max()) - topic_prob\n",
    "# STEP4. topic_idx, topic_prob을 topic_idx_list, topic_prob_list에 담는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP0. 빈 리스트 2개를 준비한다. - topic_idx_list, topic_prob_list\n",
    "topic_idx_list = []\n",
    "topic_prob_list = []\n",
    "\n",
    "# STEP1. sent_topic에서 요소 하나하나씩 뽑는다. [0.01300719, 0.976353, 0.01063981]\n",
    "for sent in sent_topic:\n",
    "    # STEP2. 이 문장을 어떤 그룹으로 분류할 건지(argmax()) - topic_idx\n",
    "    topic_idx = sent.argmax()\n",
    "\n",
    "    # STEP3. 이 문장이 분류되었을 때 그 확률(max()) - topic_prob\n",
    "    topic_prob = sent.max()\n",
    "\n",
    "    # STEP4. topic_idx, topic_prob을 topic_idx_list, topic_prob_list에 담는다.\n",
    "    topic_idx_list.append(topic_idx)\n",
    "    topic_prob_list.append(topic_prob)\n",
    "    print(sent)\n",
    "    print(f\"{topic_idx} 위치에 최댓값 {topic_prob:.4f}이 있습니다.\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 열 생성\n",
    "df[\"topic_no\"] = topic_idx_list \n",
    "df[\"topic_prob\"] = topic_prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떤 토픽이 가장 많이 있나요? - value_counts()\n",
    "df[\"topic_no\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 토픽별로 확률이 가장 높은 순서대로 보여주세요(셀 3개, 토픽 0, 1, 2각각 프린트) - sort.values()\n",
    "## 토픽 0 인 데이터 조회하기\n",
    "df.loc[ df[\"topic_no\"]==0 , :].sort_values(by=[\"topic_prob\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토픽 1 인 데이터 조회하기\n",
    "df.loc[ df[\"topic_no\"]==1 , :].sort_values(by=[\"topic_prob\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토픽 2 인 데이터 조회하기\n",
    "df.loc[ df[\"topic_no\"]==2 , :].sort_values(by=[\"topic_prob\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목적: 토픽별로 주제를 정의한다. \n",
    "# 각 토픽별로 확률이 가장 높은 5개 문장을 한번에 프린트해주세요. - 반복문\n",
    "n_topic = df[\"topic_no\"].nunique() # topic_no는 몇 개의 그룹인가요?\n",
    "print(f\"{n_topic}개의 그룹이 있습니다.\")\n",
    "\n",
    "for n in range(n_topic):\n",
    "    print(f\"# TOPIC {n}\")\n",
    "    # df에서 topic이 n인 데이터만 추출한 후, topic_prob 기준으로 내림차순 \n",
    "    topic_df = df.loc[ df[\"topic_no\"]==n , :].sort_values(by=[\"topic_prob\"], ascending=False)\n",
    "    # topic_df의 text를 가져온 후 맨 위에서 5개만 출력\n",
    "    for i in range(3):\n",
    "        print(topic_df[\"text\"].values[i])\n",
    "        print(\"-\"*100)\n",
    "    print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj01-data-analysis (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
