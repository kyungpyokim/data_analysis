{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"./data/appreply2.csv\",\n",
    "    index_col=0\n",
    ")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장바구니 분석 과정\n",
    "## 1. 지지도, 신뢰도, 향상도 지표 계산하기 (aprior 알고리즘)\n",
    "## 1) 데이터 준비 1\n",
    "## [[문장 1의 토크나이징 결과], [문장 2의 토크나이징 결과], ...]\n",
    "## 2) 데이터 준비 2\n",
    "## 예시: [[오늘], [하루, 맑음], ...]\n",
    "## 데이터 프레임\n",
    "##       오늘 하루 맑음\n",
    "## 문장1 True False ...\n",
    "## 문장2\n",
    "## 문장3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# 2. 데이터 준비하기 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 클라우드 그릴 때 [문장 1의 토크나이징 결과, 문장 2의 토크나이징 결과, ....]\n",
    "# [[문장 1의 토크나이징 결과], [문장 2의 토크나이징 결과], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from konlpy.tag import Okt \n",
    "\n",
    "okt = Okt() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "word_list = []\n",
    "stopwords = []\n",
    "\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df[\"text\"]: # sent: 문장\n",
    "    print(\"STEP1. 문장을 전처리합니다.\")\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(r\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    print(\"STEP2. 문장을 형태소분석기로 토크나이징합니다.\")\n",
    "    # 토크나이징(형태소 분석기: Okt)\n",
    "    result = okt.pos(clean_sent)\n",
    "    # result를 하나씩 뽑는다. \n",
    "    print(\"STEP3. 새로운 리스트를 만듭니다.\")\n",
    "    sub_list = []\n",
    "    print(\"\\t탐색을 시작합니다.\")\n",
    "    for res in result: # res : (단어, 품사)\n",
    "        word = res[0]\n",
    "        pos = res[1]\n",
    "        # word가 stopwords에 있으면 건너뛰기\n",
    "        if word in stopwords:\n",
    "            print(\"\\t건너뛰어!\", res)\n",
    "            continue\n",
    "\n",
    "        # sub_list 만들기: 조건 pos == \"Noun\" and len(word) > 1\n",
    "        if pos == \"Noun\" and len(word) > 1:\n",
    "            sub_list.append(word)\n",
    "        # print(f\"[RES] {res} [WORD] {word} [POS] {pos}\")\n",
    "    print(\"\\t탐색을 종료합니다.\")\n",
    "    print(f\"[SUB LIST] {sub_list}\")\n",
    "    # print(f\"[ORIGINAL] {sent}\")\n",
    "    # print(f\"[CLEAN] {clean_sent}\")\n",
    "    # print(f\"[TOKENIZING] {result}\") # 반복문으로 하나씩 확인해야 한다.\n",
    "    print(f\"STEP4. word_list에 sub_list를 추가합니다.\")\n",
    "    word_list.append(sub_list)\n",
    "    print(f\"\\t[WORD LIST] {word_list}\")\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in word_list:\n",
    "    print(x)\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 3. 데이터 준비하기 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv add mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder \n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_arr = te.fit(word_list).transform(word_list)\n",
    "te_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# 문장에 각각의 단어들이 있는지 없는지를 True / False\n",
    "df = pd.DataFrame(te_arr, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별로 출현한 횟수\n",
    "df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# 4. 장바구니 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules \n",
    "\n",
    "# support는 전체 문장 중에 itemset이 등장한 문장의 비율\n",
    "# min_support : support가 min_support 이상인것만 보여주세요\n",
    "# max_len : max_len개의 조합까지 보여주세요\n",
    "frequent_itemsets = apriori(df, min_support=0.02, use_colnames=True, max_len=2)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric이 min_threshold 이상인 것만 보여주세요.\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozenset({검색})이라고 되어있는 분들도 똑같으니 그대로 진행하시면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지지도: support(A-B) = support(B-A) = P(A∩B) = 전체 문장 중 A,B가 동시에 들어간 문장의 비율\n",
    "# 신뢰도: confidence(A-B) = P(B|A), confidence(B-A) = P(A|B)\n",
    "# 향상도: lift(A-B) = confidence(A-B)/P(A), lift(B-A) = confidence(B-A)/P(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표현: antecedents - consequents\n",
    "## 예시 1: 가게 - 개선\n",
    "## support: 전체 문장에서 \"가게\", \"개선\"이 모두 등장한 문장의 비율\n",
    "## -> 1000문장 중에 29문장은 \"가게\", \"개선\"이 같이 등장했다.\n",
    "## confidence: \"가게\"가 등장한 문장 중에서 \"개선\", \"가게\"가 동시에 등장한 문장의 비율\n",
    "## -> \"가게\" 문장이 예를 들어 100라면 100개 중 14개 정도는 \"개선\"이 들어가 있더라.\n",
    "## lift: \"가게\"가 등장했을 때와 \"가게\" 문장에서 \"개선\"이 들어가 있을 때를 비교 (우연이 아니다.)\n",
    "## -> \"가게\"가 등장할 확률보다 \"가게\" 문장들에서 \"개선\"이 들어가 있는 확률이 1.36배 높더라.\n",
    "\n",
    "## 예시 2: 개선 - 가게\n",
    "## support: 전체 문장에서 \"가게\", \"개선\"이 모두 등장한 문장의 비율\n",
    "## -> 1000문장 중에 29문장은 \"가게\", \"개선\"이 같이 등장했다.\n",
    "## confidence: \"개선\"이 등장한 문장 중에서 \"개선\", \"가게\"가 동시에 등장한 문장의 비율\n",
    "## -> \"개선\" 문장이 예를 들어 100라면 100개 중 28개 정도는 \"가게\"이 들어가 있더라.\n",
    "## lift: \"개선\"이 등장했을 때와 \"개선\" 문장에서 \"가게\"이 들어가 있을 때를 비교 (우연이 아니다.)\n",
    "## -> \"개선\"이 등장할 확률보다 \"개선\" 문장들에서 \"가게\"이 들어가 있는 확률이 1.36배 높더라."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# 5. 시각화(히트맵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[[\"antecedents\", \"consequents\", \"support\", \"confidence\", \"lift\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot_table\n",
    "pivot_data = rules.head(20).pivot_table(\n",
    "    index=\"antecedents\",        # 행\n",
    "    columns=\"consequents\",      # 열\n",
    "    values=\"lift\",              # 기준\n",
    "    fill_value=0                # 매칭되지 않는 것은 이걸로 채워라.\n",
    ")\n",
    "pivot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(pivot_data, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", linewidths=0.3, square=True)\n",
    "plt.title(\"연관성 분석 시각화(Lift 기준)\")\n",
    "plt.xlabel(\"Consequents\")\n",
    "plt.ylabel(\"Antecedents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# 6. 시각화(네트워크 분석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv add networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[[\"antecedents\",\"consequents\",\"support\",\"confidence\", \"lift\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterrows 이해하기\n",
    "# 데이터프레임에서 하나의 행씩 추출 (인덱스, 열 데이터)\n",
    "sample_data = rules[[\"antecedents\", \"consequents\"]].head(2)\n",
    "for x in sample_data.iterrows():\n",
    "    print(f\"x의 요소 개수: {len(x)}\")\n",
    "    print(x[0])\n",
    "    print(\"-\"*50)\n",
    "    print(x[1])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .join() 이해하기\n",
    "test = [\"오늘\", \"하루\", \"맑음\"]\n",
    "print(\"-\".join(test)) # ,를 다른 문자들로 바꿔보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt \n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 보여주고 싶은 데이터 설정하기(여러분이 설정하세요.)\n",
    "my_rules = rules.sort_values(by=[\"lift\"], ascending=False).head(50)\n",
    "\n",
    "# 1. 그래프 생성\n",
    "G = nx.Graph()\n",
    "\n",
    "# 2. 엣지 추가\n",
    "for _ , col_data in my_rules.iterrows():\n",
    "    # 1) 단어 추출\n",
    "    print(f\"[BEFORE] {col_data['antecedents']}, {col_data['consequents']}\")\n",
    "    antecedent = \",\".join(col_data[\"antecedents\"])\n",
    "    consequent = \",\".join(col_data[\"consequents\"])\n",
    "    print(f\"[AFTER] {antecedent}, {consequent}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "    # 2) 지표 추출\n",
    "    weight = col_data[\"lift\"]\n",
    "\n",
    "    # 3) 그래프에 정보 추가\n",
    "    G.add_edge(antecedent, consequent, weight=weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "G[\"가입\"][\"회원\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 노드 배치\n",
    "# position = nx.kamada_kawai_layout(G, scale=0.5)\n",
    "# k를 조절하면 노드간 거리를 조절할 수 있습니다.\n",
    "position = nx.spring_layout(G, k=0.9, seed=15)\n",
    "\n",
    "# 4. 가중치 추출\n",
    "scale = 0.3 ## 선의 굵기가 너무 굵다면 사이즈를 줄일 수 있습니다.\n",
    "edge_weights = [G[u][v][\"weight\"]*scale for u, v in G.edges()]\n",
    "print(edge_weights)\n",
    "\n",
    "# 5. 그리기\n",
    "plt.figure(figsize=(10,10))\n",
    "nx.draw_networkx_nodes(G, position, node_color=\"lightblue\", node_size=1000)\n",
    "nx.draw_networkx_edges(G, position, edge_color=\"gray\", width=edge_weights)\n",
    "nx.draw_networkx_labels(G, position, font_size=10, font_family=\"Malgun Gothic\")\n",
    "plt.title(\"단어 간 연관규칙 기반 네트워크 그래프(Lift 기준)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj01-data-analysis (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
