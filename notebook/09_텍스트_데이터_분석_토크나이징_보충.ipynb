{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 반복문 이해하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 불러오기\n",
    "import pandas as pd\n",
    "df_new = pd.read_csv(\n",
    "    \"./data/appreply2_보충.csv\", \n",
    "    index_col=0\n",
    ")\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표: 데이터프레임을 단어 리스트로 만드는 것\n",
    "# ['단어1', '단어2', '단어1', '단어3', ...]\n",
    "\n",
    "# 0. 빈 리스트를 만든다. word_list, stopwords\n",
    "# 1. 데이터프레임에서 text 값을 하나씩 뽑는다. -> 문장 sent\n",
    "# 2. sent에서 필요없는 문자(특수문자, 이모지 등)를 없앤다. (패턴: [^0-9a-zA-Z가-힣\\s]) -> clean sent\n",
    "# 3. 형태소 분석기로 문장을 단어 리스트로 뽑는다.(조건: Noun, 단어길이가 1보다 큰것, stopwords에 없는것) -> result\n",
    "## 3-1. (단어, 품사) 쌍의 리스트로 결과를 출력한다.\n",
    "## 3-2. 하나씩 뽑아서 품사가 Noun인지 확인한다.(반복)\n",
    "## 3-3. word가 stopwords에 있으면 건너뛴다.\n",
    "## 3-4. 품사가 Noun이고 word 길이가 1보다 큰 것을 sub list에 담는다. \n",
    "# 4. word_list에 조건에 따라 추출한 result 요소들을 추가한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "#### (1) 데이터프레임에서 반복문을 사용하여 text 열 안의 데이터를 하나씩 프린트해주세요. \n",
    "하나씩 뽑은 값은 sent에 넣는다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text 열을 준비한다. \n",
    "df_new[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "for sent in df_new[\"text\"]:\n",
    "    print(sent)\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "#### (2) sent에서 필요없는 문자(특수문자, 이모지 등)를 없앤 후, clean sent 변수에 넣는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### 정규표현식이란?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "text = \"오늘!!!!!!!       ㄴㄴㄴㄴㄴㄴ너무 ㅓㅑㅏㄴ 추워어어 very cold VERY COLD 010-0000-0040 ^^ ㅎㅇㅎㅇ\"\n",
    "\n",
    "# re.sub(패턴, 대체할값, 대상): 대상에서 \"패턴\"을 파악한후 그 값을 \"대체할 값\"으로 바꾼다.\n",
    "# 패턴: \"[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣\\s]\" : 숫자, 영어 소문자, 영어 대문자, 자음, 모음, 한글, 띄어쓰기가 아닌 것\n",
    "new_text = re.sub(\"[^0-9]\", \"\", text) # 숫자빼고 다 없애주세요\n",
    "new_text = re.sub(\"[^0-9가-힣]\", \"\", text) # 숫자,한글글자 빼고 다 없애주세요\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df_new[\"text\"]:\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    print(f\"[ORIGINAL] {sent}\")\n",
    "    print(f\"[CLEAN] {clean_sent}\")\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### (3) clean sent를 형태소 분석기로 토크나이징한 후, result에 담고 조건에 맞는 단어들만 sub_list에 취합한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "##### 형태소 분석기 사용법(Okt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt \n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"안녕하세요. 파이썬입니다.\"\n",
    "result_morphs = okt.morphs(text) # 단어만 추출\n",
    "print(result_morphs)\n",
    "result_pos = okt.pos(text) # (단어, 품사)\n",
    "print(result_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "##### 3-1. (단어, 품사) 쌍의 리스트로 결과를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df_new[\"text\"]:\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    # 토크나이징(형태소 분석기: Okt)\n",
    "    result = okt.pos(clean_sent)\n",
    "    # print(f\"[ORIGINAL] {sent}\")\n",
    "    print(f\"[CLEAN] {clean_sent}\")\n",
    "    print(f\"[TOKENIZING] {result}\") # 반복문으로 하나씩 확인해야 한다.\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "##### 3-2. 하나씩 뽑아서 품사가 Noun인지 확인한다.(반복)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df_new[\"text\"]:\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    # 토크나이징(형태소 분석기: Okt)\n",
    "    result = okt.pos(clean_sent)\n",
    "    # result를 하나씩 뽑는다. \n",
    "    for res in result:\n",
    "        word = res[0]\n",
    "        pos = res[1]\n",
    "        print(f\"[RES] {res} [WORD] {word} [POS] {pos}\")\n",
    "    # print(f\"[ORIGINAL] {sent}\")\n",
    "    # print(f\"[CLEAN] {clean_sent}\")\n",
    "    # print(f\"[TOKENIZING] {result}\") # 반복문으로 하나씩 확인해야 한다.\\\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "##### 3-3. word가 stopwords에 있으면 건너뛴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "stopwords = [\"진짜\", \"카톡\", \"로그인\"]\n",
    "\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df_new[\"text\"]:\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    # 토크나이징(형태소 분석기: Okt)\n",
    "    result = okt.pos(clean_sent)\n",
    "    # result를 하나씩 뽑는다. \n",
    "    for res in result:\n",
    "        word = res[0]\n",
    "        pos = res[1]\n",
    "        # word가 stopwords에 있으면 건너뛰기\n",
    "        if word in stopwords:\n",
    "            print(\"건너뛰어!\", res)\n",
    "            continue\n",
    "        print(f\"[RES] {res} [WORD] {word} [POS] {pos}\")\n",
    "    # print(f\"[ORIGINAL] {sent}\")\n",
    "    # print(f\"[CLEAN] {clean_sent}\")\n",
    "    # print(f\"[TOKENIZING] {result}\") # 반복문으로 하나씩 확인해야 한다.\\\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "##### 3-4. 품사가 Noun이고 word 길이가 1보다 큰 것을 sub list에 담는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "stopwords = [\"진짜\", \"카톡\", \"로그인\"]\n",
    "\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df_new[\"text\"]: # sent: 문장\n",
    "    print(\"STEP1. 문장을 전처리합니다.\")\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    print(\"STEP2. 문장을 형태소분석기로 토크나이징합니다.\")\n",
    "    # 토크나이징(형태소 분석기: Okt)\n",
    "    result = okt.pos(clean_sent)\n",
    "    # result를 하나씩 뽑는다. \n",
    "    print(\"STEP3. 새로운 리스트를 만듭니다.\")\n",
    "    sub_list = []\n",
    "    print(\"\\t탐색을 시작합니다.\")\n",
    "    for res in result: # res : (단어, 품사)\n",
    "        word = res[0]\n",
    "        pos = res[1]\n",
    "        # word가 stopwords에 있으면 건너뛰기\n",
    "        if word in stopwords:\n",
    "            print(\"\\t건너뛰어!\", res)\n",
    "            continue\n",
    "\n",
    "        # sub_list 만들기: 조건 pos == \"Noun\" and len(word) > 1\n",
    "        if pos == \"Noun\" and len(word) > 1:\n",
    "            sub_list.append(word)\n",
    "        # print(f\"[RES] {res} [WORD] {word} [POS] {pos}\")\n",
    "    print(\"\\t탐색을 종료합니다.\")\n",
    "    print(f\"[SUB LIST] {sub_list}\")\n",
    "    # print(f\"[ORIGINAL] {sent}\")\n",
    "    # print(f\"[CLEAN] {clean_sent}\")\n",
    "    # print(f\"[TOKENIZING] {result}\") # 반복문으로 하나씩 확인해야 한다.\\\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### (4). word_list에 조건에 따라 추출한 result 요소들을 추가한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "word_list = []\n",
    "stopwords = [\"진짜\", \"카톡\", \"로그인\"]\n",
    "\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df_new[\"text\"]: # sent: 문장\n",
    "    print(\"STEP1. 문장을 전처리합니다.\")\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    print(\"STEP2. 문장을 형태소분석기로 토크나이징합니다.\")\n",
    "    # 토크나이징(형태소 분석기: Okt)\n",
    "    result = okt.pos(clean_sent)\n",
    "    # result를 하나씩 뽑는다. \n",
    "    print(\"STEP3. 새로운 리스트를 만듭니다.\")\n",
    "    sub_list = []\n",
    "    print(\"\\t탐색을 시작합니다.\")\n",
    "    for res in result: # res : (단어, 품사)\n",
    "        word = res[0]\n",
    "        pos = res[1]\n",
    "        # word가 stopwords에 있으면 건너뛰기\n",
    "        if word in stopwords:\n",
    "            print(\"\\t건너뛰어!\", res)\n",
    "            continue\n",
    "\n",
    "        # sub_list 만들기: 조건 pos == \"Noun\" and len(word) > 1\n",
    "        if pos == \"Noun\" and len(word) > 1:\n",
    "            sub_list.append(word)\n",
    "        # print(f\"[RES] {res} [WORD] {word} [POS] {pos}\")\n",
    "    print(\"\\t탐색을 종료합니다.\")\n",
    "    print(f\"[SUB LIST] {sub_list}\")\n",
    "    # print(f\"[ORIGINAL] {sent}\")\n",
    "    # print(f\"[CLEAN] {clean_sent}\")\n",
    "    # print(f\"[TOKENIZING] {result}\") # 반복문으로 하나씩 확인해야 한다.\n",
    "    print(f\"STEP4. word_list에 sub_list를 추가합니다.\")\n",
    "    word_list.extend(sub_list)\n",
    "    print(f\"\\t[WORD LIST] {word_list}\")\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "word_list = []\n",
    "stopwords = [\"진짜\", \"카톡\", \"로그인\"]\n",
    "\n",
    "# 패턴: [^0-9가-힣a-zA-Z\\s]\n",
    "for sent in df_new[\"text\"]: # sent: 문장\n",
    "    print(\"STEP1. 문장을 전처리합니다.\")\n",
    "    # 리뷰 텍스트 전처리\n",
    "    clean_sent = re.sub(\"[^0-9가-힣a-zA-Z\\s]\", \"\", sent)\n",
    "    print(\"STEP2. 문장을 형태소분석기로 토크나이징합니다.\")\n",
    "    # 토크나이징(형태소 분석기: Okt)\n",
    "    result = okt.pos(clean_sent)\n",
    "    # result를 하나씩 뽑는다. \n",
    "    print(\"STEP3. 새로운 리스트를 만듭니다.\")\n",
    "    sub_list = []\n",
    "    print(\"\\t탐색을 시작합니다.\")\n",
    "    for res in result: # res : (단어, 품사)\n",
    "        word = res[0]\n",
    "        pos = res[1]\n",
    "        # word가 stopwords에 있으면 건너뛰기\n",
    "        if word in stopwords:\n",
    "            print(\"\\t건너뛰어!\", res)\n",
    "            continue\n",
    "\n",
    "        # sub_list 만들기: 조건 pos == \"Noun\" and len(word) > 1\n",
    "        if pos == \"Noun\" and len(word) > 1:\n",
    "            sub_list.append(word)\n",
    "        # print(f\"[RES] {res} [WORD] {word} [POS] {pos}\")\n",
    "    print(\"\\t탐색을 종료합니다.\")\n",
    "    print(f\"[SUB LIST] {sub_list}\")\n",
    "    # print(f\"[ORIGINAL] {sent}\")\n",
    "    # print(f\"[CLEAN] {clean_sent}\")\n",
    "    # print(f\"[TOKENIZING] {result}\") # 반복문으로 하나씩 확인해야 한다.\n",
    "    print(f\"STEP4. word_list에 sub_list를 추가합니다.\")\n",
    "    word_list.append(sub_list)\n",
    "    print(f\"\\t[WORD LIST] {word_list}\")\n",
    "    print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj01-data-analysis (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
